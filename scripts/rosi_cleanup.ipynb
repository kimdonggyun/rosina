{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROSINA data\n",
    "# remove rosina artefact\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN\n",
    "from kneed import KneeLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath\n",
    "rosinapath = \"/Users/dkim/Desktop/work/MSM107_analysis/data/06_ROSINA_new\" # path for the ROSINA data\n",
    "ctdfile = \"/Users/dkim/Desktop/work/MSM107_analysis/data/04_CTD-Idronaut/msm107_ctd.xlsx\" # path for the CTD data\n",
    "plotpath = \"/Users/dkim/Desktop/work/MSM107_analysis/plots\" # path for the plots\n",
    "ctdpath = \"/Users/dkim/Desktop/work/MSM107_analysis/data/04_CTD-Idronaut\" # path for the CTD data\n",
    "\n",
    "# read binned particle data\n",
    "particle_bin = [\"Rosi01/Profile01_BinnedData_220909-0901.txt\", \"Rosi02/Profile02_BinnedData_220909-1310.txt\", \"Rosi03/Profile03_BinnedData_220909-1328.txt\",\n",
    "\"Rosi04/Profile04_BinnedData_220909-1404.txt\", \"Rosi05/Profile05_BinnedData_220909-1842.txt\", \"Rosi06/Profile06_BinnedData_220912-1017.txt\",\n",
    "\"Rosi07/Profile07_BinnedData_220910-0846.txt\", \"Rosi08/Profile08_BinnedData_220911-0857.txt\", \"Rosi09/Profile09_BinnedData_220911-1547.txt\",\n",
    "\"Rosi10/Profile10_BinnedData_220909-1508.txt\", \"Rosi11/Profile11_BinnedData_220911-1601.txt\", \"Rosi12/Profile12_BinnedData_220912-1135.txt\",\n",
    "\"Rosi13/Profile13_BinnedData_220912-1205.txt\", \"Rosi14/Profile14_BinnedData_220914-1313.txt\", \"Rosi15/Profile15_BinnedData_220914-1715.txt\",\n",
    "\"Rosi16/Profile16_BinnedData_220915-0824.txt\", \"Rosi17/Profile17_BinnedData_220915-0924.txt\", \"Rosi18/Profile18_BinnedData_220915-1235.txt\",\n",
    "\"Rosi19/Profile19_BinnedData_220915-1349.txt\", \"Rosi20/Profile20_BinnedData_220915-1413.txt\"\n",
    "]\n",
    "\n",
    "# read particle image property data\n",
    "particle_img = [\"Rosi01/Profile01_ProcessingLogFile_220909-0901.txt\", \"Rosi02/Profile02_ProcessingLogFile_220909-1310.txt\", \"Rosi03/Profile03_ProcessingLogFile_220909-1328.txt\",\n",
    "\"Rosi04/Profile04_ProcessingLogFile_220909-1404.txt\", \"Rosi05/Profile05_ProcessingLogFile_220909-1842.txt\", \"Rosi06/Profile06_ProcessingLogFile_220912-1017.txt\",\n",
    "\"Rosi07/Profile07_ProcessingLogFile_220910-0846.txt\", \"Rosi08/Profile08_ProcessingLogFile_220911-0857.txt\", \"Rosi09/Profile09_ProcessingLogFile_220911-1547.txt\",\n",
    "\"Rosi10/Profile10_ProcessingLogFile_220909-1508.txt\", \"Rosi11/Profile11_ProcessingLogFile_220911-1601.txt\", \"Rosi12/Profile12_ProcessingLogFile_220912-1135.txt\",\n",
    "\"Rosi13/Profile13_ProcessingLogFile_220912-1205.txt\", \"Rosi14/Profile14_ProcessingLogFile_220914-1313.txt\", \"Rosi15/Profile15_ProcessingLogFile_220914-1715.txt\",\n",
    "\"Rosi16/Profile16_ProcessingLogFile_220915-0824.txt\", \"Rosi17/Profile17_ProcessingLogFile_220915-0924.txt\", \"Rosi18/Profile18_ProcessingLogFile_220915-1235.txt\",\n",
    "\"Rosi19/Profile19_ProcessingLogFile_220915-1349.txt\", \"Rosi20/Profile20_ProcessingLogFile_220915-1413.txt\"\n",
    "]\n",
    "\n",
    "# read CTD data\n",
    "CTD_station_list = [\n",
    "\"01-Profile01/MSM107-P001-C02.txt\", \"02-Profile03/MSM107-P003-C01.txt\",\n",
    "\"03-Profile04/MSM107-P004-C01.txt\", \"04-Profile05/MSM107-P005-C01.txt\",\n",
    "\"05-Profile06/MSM107-P006-C01.txt\", \"06-Profile07/MSM107-P007-C03.txt\",\n",
    "\"07-Profile08/MSM107-P008-C01.txt\", \"08-Profile10/MSM107-P010-C01.txt\",\n",
    "\"09-Profile11/MSM107-P011-C01.txt\", \"10-Profile12/MSM107-P012-C03.txt\", \"11-Profile13/MSM107-P013-C03.txt\",\n",
    "\"12-Profile14/MSM107-P014-C01.txt\", \"13-Profile15/MSM107-P015-C01.txt\",\n",
    "\"14-Profile16/MSM107-P016-C01.txt\", \"15-Profile17/MSM107-P017-C02.txt\",\n",
    "\"16-Profile18/MSM107-P018-C01.txt\", \"17-Profile19/MSM107-P019-C01.txt\",\n",
    "\"18-Profile20/MSM107-P020-C02.T\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X and Y cord is overlaping in % 94.68864468864469\n",
      "65.88053780030857 percentage of images are remained from orginal data\n",
      "total number of images to be removed is 1548\n",
      "X and Y cord is overlaping in % 57.96812749003985\n",
      "86.4470588235294 percentage of images are remained from orginal data\n",
      "total number of images to be removed is 288\n",
      "X and Y cord is overlaping in % 27.9445727482679\n",
      "89.29856115107914 percentage of images are remained from orginal data\n",
      "total number of images to be removed is 119\n",
      "X and Y cord is overlaping in % 91.01876675603218\n",
      "63.913392141138736 percentage of images are remained from orginal data\n",
      "total number of images to be removed is 1350\n",
      "X and Y cord is overlaping in % 96.13537617196432\n",
      "52.65891385344925 percentage of images are remained from orginal data\n",
      "total number of images to be removed is 4193\n",
      "X and Y cord is overlaping in % 89.62406015037594\n",
      "71.38728323699422 percentage of images are remained from orginal data\n",
      "total number of images to be removed is 594\n",
      "X and Y cord is overlaping in % 92.77815391559467\n",
      "63.59197443181818 percentage of images are remained from orginal data\n",
      "total number of images to be removed is 4101\n",
      "X and Y cord is overlaping in % 96.18055555555556\n",
      "30.064450903576397 percentage of images are remained from orginal data\n",
      "total number of images to be removed is 11068\n",
      "X and Y cord is overlaping in % 37.68115942028986\n",
      "87.52079866888519 percentage of images are remained from orginal data\n",
      "total number of images to be removed is 75\n",
      "X and Y cord is overlaping in % 86.66666666666667\n",
      "64.8780487804878 percentage of images are remained from orginal data\n",
      "total number of images to be removed is 720\n",
      "X and Y cord is overlaping in % 94.76479514415782\n",
      "45.87155963302752 percentage of images are remained from orginal data\n",
      "total number of images to be removed is 1239\n",
      "X and Y cord is overlaping in % 90.31620553359684\n",
      "49.94475138121547 percentage of images are remained from orginal data\n",
      "total number of images to be removed is 453\n",
      "X and Y cord is overlaping in % 91.95097714474991\n",
      "38.43416370106761 percentage of images are remained from orginal data\n",
      "total number of images to be removed is 2768\n",
      "X and Y cord is overlaping in % 97.39089920795155\n",
      "47.500420804578354 percentage of images are remained from orginal data\n",
      "total number of images to be removed is 6238\n",
      "X and Y cord is overlaping in % 66.32860040567951\n",
      "72.96389588581025 percentage of images are remained from orginal data\n",
      "total number of images to be removed is 322\n",
      "X and Y cord is overlaping in % 59.47467166979362\n",
      "85.22674146797569 percentage of images are remained from orginal data\n",
      "total number of images to be removed is 316\n",
      "X and Y cord is overlaping in % 95.57130203720106\n",
      "73.98962480395706 percentage of images are remained from orginal data\n",
      "total number of images to be removed is 2156\n",
      "X and Y cord is overlaping in % 13.703703703703704\n",
      "94.99252615844544 percentage of images are remained from orginal data\n",
      "total number of images to be removed is 67\n",
      "X and Y cord is overlaping in % 21.818181818181817\n",
      "93.14472252448314 percentage of images are remained from orginal data\n",
      "total number of images to be removed is 63\n",
      "X and Y cord is overlaping in % 96.52076318742986\n",
      "62.08653278204371 percentage of images are remained from orginal data\n",
      "total number of images to be removed is 5135\n"
     ]
    }
   ],
   "source": [
    "ctddepth = pd.read_excel(ctdfile)\n",
    "bins = [0.04875, 0.061425, 0.0773955, 0.0975183, 0.122873, 0.15482, 0.195073, 0.245792, 0.309698, 0.39022, 0.491677, 0.619513,\n",
    "        0.780587, 0.983539, 1.23926, 1.56147, 1.96745, 2.47898, 3.12352, 3.93564, 5]\n",
    "params = [\"ImgNo\",  \"ActContour\",  \"NoOfContours\",  \"EquiDia\",  \"AspectRatio\",  \"CumTotArea\",  \"Area\",  \"CumTotVol\",\n",
    "  \"Volume\",  \"RectX\",  \"RectY\",  \"RectWidth\",  \"RectHeight\",  \"MinRectWidth\",  \"MinRectHeight\",  \"EdgeContact\",  \"Perimeter\",\n",
    "    \"Circularity\",  \"Elongation\",  \"Major\",  \"Feret\",  \"Mean\",   \"StdDev\",  \"Intden\",  \"Range\",  \"Meanpos\",  \"Cv\",  \"Sr\",  \"Median\",\n",
    "      \"Modal\",  \"BckgrdMean\",  \"BckgrdStdDev\" ]\n",
    "profile2run = [\"Profile01\", \"Profile02\", \"Profile03\", \"Profile04\", \"Profile05\", \"Profile06\",\n",
    "               \"Profile07\", \"Profile08\", \"Profile09\", \"Profile10\", \"Profile11\", \"Profile12\",\n",
    "               \"Profile13\", \"Profile14\", \"Profile15\", \"Profile16\", \"Profile17\", \"Profile18\",\n",
    "               \"Profile19\", \"Profile20\"] # profilename\n",
    "\n",
    "for p in profile2run:\n",
    "    #ctd_idx = [idx for idx, s in enumerate(CTD_station_list) if p in s][0] # ctd file index\n",
    "    p_idx = [idx for idx, s in enumerate(particle_bin) if p in s][0] # particle file index\n",
    "    i_idx = [idx for idx, s in enumerate(particle_img) if p in s][0] # img file index\n",
    "    profile = particle_bin[p_idx].split(os.sep)[0] # rosina profile number e.g. Rosi01\n",
    "    bindata = pd.read_csv(os.path.expanduser( os.path.join(rosinapath, particle_bin[i_idx]) ), sep=\"\\s+\", skiprows=24, header=0,index_col=False, engine=\"python\")\n",
    "    imgdata = pd.read_csv(os.path.expanduser( os.path.join(rosinapath, particle_img[i_idx]) ),sep=\"\\s+\", skiprows=23,index_col=False, names=params, engine=\"python\")\n",
    "    #ctddf = pd.read_csv(os.path.expanduser( os.path.join(ctdpath, CTD_station_list[ctd_idx]) ), sep=\"\\t\", skiprows=16, header=0, engine=\"python\")\n",
    "    \n",
    "    ## particle volume and number data into separate dataframe\n",
    "    numdf = bindata.iloc[:, 1:22] # particle number df\n",
    "    numdf.columns = bins\n",
    "    areadf = bindata.iloc[:, 22:43] # particle area df\n",
    "    areadf.columns = bins\n",
    "    voldf = bindata.iloc[:, 43:64]  # particle volumn df\n",
    "    voldf.columns = bins\n",
    "\n",
    "    ## add depth information based on the sdepth and edepth from CTD and Imagenumber\n",
    "    imgno = bindata.iloc[:, 0].to_list() # image no\n",
    "    sdepth, edepth = ctddepth[\"sdepth\"].loc[ctddepth[\"profile\"] == profile].values[0], ctddepth[\"edepth\"].loc[ctddepth[\"profile\"] == profile].values[0]\n",
    "    dinterval =  (edepth-sdepth) / len(imgno) # interval between the img profile no\n",
    "    depth = np.arange(sdepth, edepth, dinterval)[:len(imgno)]\n",
    "    \n",
    "    # append depth and imgno info to binned particle data \n",
    "    numdf[\"depth\"] = depth\n",
    "    voldf[\"depth\"] = depth\n",
    "    areadf[\"depth\"] = depth\n",
    "    numdf[\"ImgNo\"] = bindata.iloc[:, 0]\n",
    "    voldf[\"ImgNo\"] = bindata.iloc[:, 0]\n",
    "    areadf[\"ImgNo\"] = bindata.iloc[:, 0]\n",
    "\n",
    "    # append depth info to image data\n",
    "    dimgdf = pd.DataFrame({\"depth\":depth, \"ImgNo\":imgno})\n",
    "    imgdf = imgdata.merge(dimgdf, on=\"ImgNo\", how=\"left\")\n",
    "\n",
    "    # step 0. define the x and y position of the image\n",
    "    xposition = imgdf[\"RectX\"]\n",
    "    yposition = imgdf[\"RectY\"]\n",
    "    # save a figure of the x and y position in scatter plot with histogram\n",
    "    scatter_axes = plt.subplot2grid((3, 3), (1, 0), rowspan=2, colspan=2)\n",
    "    x_hist_axes = plt.subplot2grid((3, 3), (0, 0), colspan=2, sharex=scatter_axes)\n",
    "    y_hist_axes = plt.subplot2grid((3, 3), (1, 2), rowspan=2, sharey=scatter_axes)\n",
    "    scatter_axes.scatter(xposition, yposition, color=\"black\", s=0.5, alpha=0.7)\n",
    "    x_hist_axes.hist(xposition, bins= 50, color=\"black\")\n",
    "    y_hist_axes.hist(yposition, bins= 50,  orientation='horizontal', color=\"black\")\n",
    "    scatter_axes.set_xlabel(\"x position\")\n",
    "    scatter_axes.set_ylabel(\"y position\")\n",
    "    x_hist_axes.tick_params(axis=\"both\", labelbottom =False, labeltop=False, labelleft=False, labelright=False)\n",
    "    y_hist_axes.tick_params(axis=\"both\", labelbottom=False, labeltop=False, labelleft=False, labelright=False)\n",
    "    plt.savefig(os.path.expanduser( os.path.join(plotpath, str(profile+\"before_xycoodrdination\")) ), dpi=600, facecolor=\"white\")\n",
    "    plt.close()\n",
    "\n",
    "    # find frequency of each pixel coordination\n",
    "    xposcount = xposition.value_counts()[:-1].values # count of each pixel coordination\n",
    "    xposcord = xposition.value_counts()[:-1].index # pixel coordination of image\n",
    "    yposcount = yposition.value_counts()[:-1].values # count of each pixel coordination\n",
    "    yposcord = yposition.value_counts()[:-1].index # pixel coordination of image\n",
    "\n",
    "    # step 1. find the optimal bin number for the histogram. based on the Feedman-Diaconis <- I will skip this method for now\n",
    "    # instead, I will set the histogram bin numer to 50 as it is working well so far.\n",
    "    #bin_num = (max(poscountnorm) - min(poscountnorm))  /( ( 2*stats.iqr(poscountnorm) ) / (len(poscountnorm)**(1/3)) ) <- this is the Feedan-Diaconis function to find the best histgram bin numnbers\n",
    "    xcounts, xcbins = np.histogram(xposcount, 50) # count is height of the histogram, bins is the bin width. i.g, normalized count\n",
    "    xcountidx = np.where(xposcount > xcbins[1]) # this is the index of the \"xposcount\" having high number of counts\n",
    "    ycounts, ycbins = np.histogram(yposcount, 50) # count is height of the histogram, bins is the bin width. i.g, normalized count\n",
    "    ycountidx = np.where(yposcount > ycbins[1]) # this is the index of the \"yposcount\" having high number of counts\n",
    "\n",
    "    \"\"\"\n",
    "    # in this part, I will define the duplicated images.\n",
    "    # First, I will use the index having high frequent X and Y coordination from the step 1.\n",
    "    # We assume all the images selected from the step 1 as duplicated images.\n",
    "    # And then we will examine if those images are really duplicated images or not following the next steps.\n",
    "    # Second, I will use image parametere \"AspectRatio\" which returns the the ratio of shortest and longest\n",
    "    # Third, I will use \"Perimeter\" and \"Eauivalent diameter\"\n",
    "    \"\"\"\n",
    "    ####################################\n",
    "    # workging with X and Y coordination\n",
    "    IX = imgdf.index[ imgdf[\"RectX\"].isin(xposcord[xcountidx]) == True].tolist() # index of freqent coordination in the original img dataframe\n",
    "    IY = imgdf.index[ imgdf[\"RectY\"].isin(yposcord[ycountidx]) == True].tolist() # index of freqent coordination in the original img dataframe\n",
    "    # I will have a look at how many duplicated X and Y coordination is \n",
    "    uniqXidx = set(IX) # unique index of duplicated x cord\n",
    "    uniqYidx = set(IY) # unique index of duplicated y cord\n",
    "    overlapidx = list(uniqXidx & uniqYidx) # overlapping index\n",
    "    universalidx = list(uniqXidx | uniqYidx) # universal index i.e., all indexes from x coord and y coord\n",
    "    print(\"X and Y cord is overlaping in %\" ,float(len(overlapidx)) / len(universalidx) * 100) # it returns how many of duplicated x and y coordinates are positioned in the same row\n",
    "    # Now, I will use overlapping index to remove the duplicated images\n",
    "\n",
    "    ####################################\n",
    "    # I will use the following parameters to identify the duplicated images\n",
    "    A = imgdf[\"AspectRatio\"][overlapidx] # aspec ratio of img from frequent coordination\n",
    "    X = imgdf[\"Perimeter\"][overlapidx] / imgdf[\"EquiDia\"][overlapidx] # perimeter / equivalent diameter\n",
    "    E = imgdf[\"Elongation\"][overlapidx] # elongation\n",
    "    M = imgdf[\"Mean\"][overlapidx] # gray mean level of the image\n",
    "\n",
    "\n",
    "    ####################################\n",
    "    \"\"\"\n",
    "    I will use Density Based Spatial Clustering of Applications with Noise (DBSCAN) to detect the\n",
    "    clusters and noises. The clusters are going to be the duplicated images and noises is going to be non-duplicated images\n",
    "    to use this clustering method, two parameters should be identified.\n",
    "    Radius of neighborhoods and minimum number of data points in a give radius neighborhood .\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # 1. find best radius\n",
    "    df = pd.DataFrame({\"M\": M, \"X\":X, \"E\":E}) # create dataframe of parameters\n",
    "    minsample = 6\n",
    "    nbrs = NearestNeighbors(n_neighbors= minsample+1).fit(df) # find the best radius value for DBSCAN\n",
    "    neigh_dist, neigh_ind = nbrs.kneighbors(df)\n",
    "    sort_neigh_dist = np.sort(neigh_dist, axis = 0)\n",
    "    k_dist = sort_neigh_dist[:, 4]\n",
    "    kneedle = KneeLocator(x= range(1, len(neigh_dist)+1), y = k_dist, S= 1.0,\n",
    "                          curve=\"concave\", direction=\"increasing\", online=True)\n",
    "    r = kneedle.knee_y # radius of neighborhood\n",
    "    cluster = DBSCAN(eps=r, min_samples=minsample).fit(df) # DBSCAN clustering\n",
    "    #print(set(cluster.labels_))\n",
    "\n",
    "    # plot and save the result of DBSCAN\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection=\"3d\")\n",
    "    colors = [\"yellow\", \"red\", \"purple\", \"black\", \"blue\", \"green\", \"pink\"]\n",
    "    scatter = ax.scatter(df[\"M\"], df[\"X\"], df[\"E\"], c=cluster.labels_, s=5, alpha=0.4, label=colors[0: len(cluster.labels_)])\n",
    "    ax.legend(handles=scatter.legend_elements()[0])\n",
    "    fig.savefig(os.path.join(plotpath, str(profile+\"DBSCAN\")), dpi=600, facecolor=\"white\")\n",
    "    plt.close()\n",
    "\n",
    "    # remove outliers from DBSCAN. Again, Outliers are non duplicated images and clusterd point is duplicated images\n",
    "    nondupidx = np.where(cluster.labels_ == -1)[0].tolist()\n",
    "    for index in sorted(nondupidx, reverse=True):\n",
    "      del overlapidx[index]\n",
    "\n",
    "    ############################################################################################\n",
    "    # now get rid of the rows coming from the stuck object from image property dataframe\n",
    "    original_img_len = len(imgdf.index)\n",
    "    newimgdf = imgdf.drop(overlapidx, axis=0) # remove the rows from the original dataframe of images properties\n",
    "    filtered_img_len = len(newimgdf.index)\n",
    "    print((filtered_img_len/original_img_len)*100, \"percentage of images are remained from orginal data\" ) # print the percentage of images remained from the original dataframe\n",
    "    print(\"total number of images to be removed is\", original_img_len - filtered_img_len) # print the number of images removed from the original dataframe\n",
    "    # save the new dataframe of images properties\n",
    "    newimgdf.to_csv(os.path.expanduser( os.path.join(rosinapath, particle_img[i_idx].replace(p, str(\"filtered_\"+p))) ), index=False)\n",
    "\n",
    "\n",
    "    # save figure of the x and y position of the images from the new dataframe\n",
    "    newxposition = newimgdf[\"RectX\"]\n",
    "    newyposition = newimgdf[\"RectY\"]\n",
    "    scatter_axes = plt.subplot2grid((3, 3), (1, 0), rowspan=2, colspan=2)\n",
    "    x_hist_axes = plt.subplot2grid((3, 3), (0, 0), colspan=2, sharex=scatter_axes)\n",
    "    y_hist_axes = plt.subplot2grid((3, 3), (1, 2), rowspan=2, sharey=scatter_axes)\n",
    "    scatter_axes.scatter(newxposition, newyposition, color=\"black\", s=0.5, alpha=0.7)\n",
    "    x_hist_axes.hist(newxposition, bins= 50, color=\"black\")\n",
    "    y_hist_axes.hist(newyposition, bins= 50,  orientation='horizontal', color=\"black\")\n",
    "    scatter_axes.set_xlabel(\"x position\")\n",
    "    scatter_axes.set_ylabel(\"y position\")\n",
    "    x_hist_axes.tick_params(axis=\"both\", labelbottom =False, labeltop=False, labelleft=False, labelright=False)\n",
    "    y_hist_axes.tick_params(axis=\"both\", labelbottom=False, labeltop=False, labelleft=False, labelright=False)\n",
    "    plt.savefig(os.path.expanduser( os.path.join(plotpath, str(profile+\"after_xycoodrdination\")) ), dpi=600, facecolor=\"white\")\n",
    "    plt.close()\n",
    "\n",
    "    ##################################################################################\n",
    "    # create figures\n",
    "    # imgdf: has information of each cropped images and corresponding depth information is appended\n",
    "    fig, ax = plt.subplots(4,4, figsize=(12, 12), facecolor=\"white\", )\n",
    "    fig.suptitle(profile) # plot name\n",
    "\n",
    "    ## particle image properties \n",
    "    ax1 = ax[0,0] # circularity distribution from original data\n",
    "    ax1.scatter(imgdf[\"Circularity\"], imgdf[\"depth\"], s=0.5, alpha=0.3, color=\"black\")\n",
    "    ax1.set_xlabel(\"Circularity\")\n",
    "    ax1.set_ylabel(\"Depth (m)\")\n",
    "    ax1.set_xlim(0, 1.2)\n",
    "    ax1.invert_yaxis()\n",
    "\n",
    "    ax7 = ax[0,1] # Circularity distribution from filtered data\n",
    "    ax7.scatter(newimgdf[\"Circularity\"], newimgdf[\"depth\"], s=0.5, alpha=0.3, color=\"black\")\n",
    "    ax7.set_xlabel(\"Circularity\")\n",
    "    ax7.set_xlim(0, 1.2)\n",
    "    ax7.invert_yaxis()\n",
    "\n",
    "    ax2 = ax[0,2] # Perimeter distribution from original data\n",
    "    ax2.scatter(imgdf[\"Perimeter\"], imgdf[\"depth\"], s=0.5, alpha=0.3, color=\"black\")\n",
    "    ax2.set_xlabel(\"Perimeter\")\n",
    "    ax2.set_xlim(0, 5)\n",
    "    ax2.invert_yaxis()\n",
    "\n",
    "    ax8 = ax[0,3] # Perimeter distribution from filtered data\n",
    "    ax8.scatter(newimgdf[\"Perimeter\"], newimgdf[\"depth\"], s=0.5, alpha=0.3, color=\"black\")\n",
    "    ax8.set_xlabel(\"Perimeter\")\n",
    "    ax8.set_xlim(0, 5)\n",
    "    ax8.invert_yaxis()\n",
    "\n",
    "    ax3 = ax[1,0] # Mean distribution from original data\n",
    "    ax3.scatter(imgdf[\"Mean\"], imgdf[\"depth\"], s=0.5, alpha=0.3, color=\"black\")\n",
    "    ax3.set_xlabel(\"Mean Grey Level\")\n",
    "    ax3.set_ylabel(\"Depth (m)\")\n",
    "    ax3.invert_yaxis()\n",
    "\n",
    "    ax9 = ax[1,1] # Mean distribution from filtered data\n",
    "    ax9.scatter(newimgdf[\"Mean\"], newimgdf[\"depth\"], s=0.5, alpha=0.3, color=\"black\")\n",
    "    ax9.set_xlabel(\"Mean Grey Level\")\n",
    "    ax9.invert_yaxis()\n",
    "\n",
    "    ax4 = ax[1,2] # StdDev distribution from original data\n",
    "    ax4.scatter(imgdf[\"StdDev\"], imgdf[\"depth\"], s=0.5, alpha=0.3, color=\"black\")\n",
    "    ax4.set_xlabel(\"StdDev Grey level\")\n",
    "    ax4.invert_yaxis()\n",
    "\n",
    "    ax10 = ax[1,3] # StdDev distribution from filtered data\n",
    "    ax10.scatter(newimgdf[\"StdDev\"], newimgdf[\"depth\"], s=0.5, alpha=0.3, color=\"black\")\n",
    "    ax10.set_xlabel(\"StdDev Grey level\")\n",
    "    ax10.invert_yaxis()\n",
    "\n",
    "    ax5 = ax[2,0] # Elongation distribution from original data\n",
    "    ax5.scatter(imgdf[\"Elongation\"], imgdf[\"depth\"], s=0.5, alpha=0.3, color=\"black\")\n",
    "    ax5.set_xlabel(\"Elongation\")\n",
    "    ax5.set_ylabel(\"Depth (m)\")\n",
    "    ax5.invert_yaxis()\n",
    "\n",
    "    ax11 = ax[2,1] # Elongation distribution from filtered data\n",
    "    ax11.scatter(newimgdf[\"Elongation\"], newimgdf[\"depth\"], s=0.5, alpha=0.3, color=\"black\")\n",
    "    ax11.set_xlabel(\"Elongation\")\n",
    "    ax11.invert_yaxis()\n",
    "\n",
    "    ax6 = ax[2,2] # Area distribution from original data\n",
    "    ax6.scatter(imgdf[\"Area\"], imgdf[\"depth\"], s=0.5, alpha=0.3, color=\"black\")\n",
    "    ax6.set_xlabel(\"Area\")\n",
    "    ax6.set_xlim(-0.02, 0.5)\n",
    "    ax6.invert_yaxis()\n",
    "\n",
    "    ax12 = ax[2,3] # Area distribution from original data\n",
    "    ax12.scatter(newimgdf[\"Area\"], newimgdf[\"depth\"], s=0.5, alpha=0.3, color=\"black\")\n",
    "    ax12.set_xlabel(\"Area\")\n",
    "    ax12.set_xlim(-0.02, 0.5)\n",
    "    ax12.invert_yaxis()\n",
    "\n",
    "    ax13 = ax[3,0] # ESD distribution from filtered data\n",
    "    ax13.scatter(imgdf[\"EquiDia\"], imgdf[\"depth\"], s=0.5, alpha=0.3, color=\"black\")\n",
    "    ax13.set_xlabel(\"ESD\")\n",
    "    ax13.set_ylabel(\"Depth (m)\")\n",
    "    ax13.invert_yaxis()\n",
    "\n",
    "    ax14 = ax[3,1] # ESD distribution from original data\n",
    "    ax14.scatter(newimgdf[\"EquiDia\"], newimgdf[\"depth\"], s=0.5, alpha=0.3, color=\"black\")\n",
    "    ax14.set_xlabel(\"ESD\")\n",
    "    ax14.set_ylabel(\"Depth (m)\")\n",
    "    ax14.invert_yaxis()\n",
    "\n",
    "    ax15 = ax[3,2] # number of particles distribution from filtered data in each 10m depth bin in histogram\n",
    "    hist_o = np.histogram(imgdf[\"depth\"], range=(np.nanmin(imgdf[\"depth\"]), np.nanmax(imgdf[\"depth\"])),\n",
    "                            bins=int(imgdf[\"depth\"].max()/10) + 1)\n",
    "    ax15.plot(hist_o[0], hist_o[1][:-1], color=\"black\")\n",
    "    ax15.set_xlabel(\"Number of particles\")\n",
    "    ax15.set_ylabel(\"Depth (m)\")\n",
    "    ax15.invert_yaxis()\n",
    "\n",
    "    ax16 = ax[3,3] # number of particles distribution from original data in each 10m depth bin in histogram\n",
    "    hist_n = np.histogram(newimgdf[\"depth\"], range=(np.nanmin(newimgdf[\"depth\"]), np.nanmax(newimgdf[\"depth\"])),\n",
    "                           bins=int(newimgdf[\"depth\"].max()/10) + 1)\n",
    "    ax16.plot(hist_n[0], hist_n[1][:-1], color=\"black\")\n",
    "    ax16.set_xlabel(\"Number of particles\")\n",
    "    ax16.set_ylabel(\"Depth (m)\")\n",
    "    ax16.invert_yaxis()\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(plotpath, str(profile+\"DBSCAN_filter_img\")), dpi=300, facecolor=\"white\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ## CTD data\n",
    "    ax7 = ax[0,2]\n",
    "    ax7.scatter(ctddf[\"Temperature\"], ctddf[\"Depth\"], s=0.5, alpha=0.3, color=\"black\")\n",
    "    ax7.set_xlabel(\"Temperature\")\n",
    "    #ax7.set_xlim(-0.02, 0.5)\n",
    "    ax7.invert_yaxis()\n",
    "\n",
    "    ax8 = ax[0,3]\n",
    "    ax8.scatter(ctddf[\"Salinity\"], ctddf[\"Depth\"], s=0.5, alpha=0.3, color=\"black\")\n",
    "    ax8.set_xlabel(\"Salinity\")\n",
    "    #ax8.set_xlim(-0.02, 0.5)\n",
    "    ax8.invert_yaxis()\n",
    "\n",
    "    ax9 = ax[1,2]\n",
    "    ax9.scatter(ctddf[\"Optical O2%\"], ctddf[\"Depth\"], s=0.5, alpha=0.3, color=\"black\")\n",
    "    ax9.set_xlabel(\"Oxygen\")\n",
    "    #ax9.set_xlim(-0.02, 0.5)\n",
    "    ax9.invert_yaxis()\n",
    "\n",
    "    ax10 = ax[1,3]\n",
    "    ax10.scatter(ctddf[\"Fluorometer AutoScale\"], ctddf[\"Depth\"], s=0.5, alpha=0.3, color=\"black\")\n",
    "    ax10.set_xlabel(\"Fluorescence\")\n",
    "    #ax10.set_xlim(-0.02, 0.5)\n",
    "    ax10.invert_yaxis()\n",
    "\n",
    "    ## particle data\n",
    "    ax11 = ax[2,2]\n",
    "    ax11.scatter(numdf.iloc[:, 0:-1].sum(axis=1), numdf[\"depth\"], s=0.5, alpha=0.3, color=\"black\")\n",
    "    ax11.set_xlabel(\"total particle nunber (#/L)\")\n",
    "    ax11.set_xlim(0, 300)\n",
    "    ax11.invert_yaxis()\n",
    "\n",
    "    ax12 = ax[2,3]\n",
    "    ax12.scatter(voldf.iloc[:, 0:-1].sum(axis=1), voldf[\"depth\"], s=0.5, alpha=0.3, color=\"black\")\n",
    "    ax12.set_xlabel(\"total particle volume ppm\")\n",
    "    ax12.set_xlim(0, 0.5)\n",
    "    ax12.invert_yaxis()\n",
    "    \"\"\"\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f956103ecc23466afb033aec94ec549118fe609ff0725e75a01482599677d77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
